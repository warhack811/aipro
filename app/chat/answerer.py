"""
Mami AI - YanÄ±t Ãœretici (Groq Answerer)
=======================================

Bu modÃ¼l, Groq API kullanarak yÃ¼ksek kaliteli yanÄ±tlar Ã¼retir.

Ã–zellikler:
    - Dinamik temperature (domain/risk bazlÄ±)
    - Chain-of-Thought desteÄŸi (karmaÅŸÄ±k sorular iÃ§in)
    - Context injection (RAG, hafÄ±za)
    - Streaming desteÄŸi
    - Thinking block filtreleme

KullanÄ±m:
    from app.chat.answerer import generate_answer, generate_answer_stream
    
    # Tek seferlik yanÄ±t
    answer = await generate_answer(
        message="Python'da liste nasÄ±l oluÅŸturulur?",
        context="KullanÄ±cÄ± yeni baÅŸlayan bir geliÅŸtirici"
    )
    
    # Streaming yanÄ±t
    async for chunk in generate_answer_stream(message, context):
        print(chunk, end="")
"""

from __future__ import annotations

import logging
import re
from typing import Any, AsyncGenerator, Dict, List, Optional, cast

# ModÃ¼l logger'Ä±
logger = logging.getLogger(__name__)

# =============================================================================
# SÄ°STEM PROMPT
# =============================================================================

SYSTEM_PROMPT_UNIVERSAL = """
Sen Mami AI'sÄ±n - profesyonel, zeki ve kullanÄ±cÄ± odaklÄ± bir yapay zeka asistanÄ±sÄ±n.

## DÃœÅÃœNME SÃœRECÄ°
1. KullanÄ±cÄ±nÄ±n gerÃ§ek niyetini anla (ne soruyor, ne istiyor)
2. BaÄŸlamdaki kullanÄ±cÄ± bilgilerini (isim, tercihler, geÃ§miÅŸ) cevaba yedir
3. AÃ§Ä±k, net ve deÄŸer katan bir cevap oluÅŸtur

## TÃœRKÃ‡E KALÄ°TESÄ° KURALLARI (KRÄ°TÄ°K!) ğŸ‡¹ğŸ‡·
- **TAM CÃœMLELER:** Her cÃ¼mle mutlaka tamamlanmalÄ±, yarÄ±m kalmamalÄ±. Nokta, soru iÅŸareti veya Ã¼nlem ile bitmeli.
- **DÄ°LBÄ°LGÄ°SÄ°:** TÃ¼rkÃ§e dilbilgisi kurallarÄ±na uy (ekler, Ã§oÄŸul, zamanlar, bÃ¼yÃ¼k/kÃ¼Ã§Ã¼k harf).
- **DOÄAL TÃœRKÃ‡E:** Robotik kalÄ±p ifadelerden kaÃ§Ä±n, doÄŸal konuÅŸ. "Size nasÄ±l yardÄ±mcÄ± olabilirim?" gibi kliÅŸeler kullanma.
- **KOD AÃ‡IKLAMALARI:** Kod Ã¶rnekleri verirken aÃ§Ä±klamalarÄ± TAM ve ANLAÅILIR TÃ¼rkÃ§e yaz. YarÄ±m cÃ¼mleler olmamalÄ±.
- **NOKTALAMA:** Noktalama iÅŸaretlerini doÄŸru kullan (nokta, virgÃ¼l, soru iÅŸareti, Ã¼nlem).
- **KELÄ°ME SEÃ‡Ä°MÄ°:** Uygun TÃ¼rkÃ§e kelimeler kullan, gereksiz Ä°ngilizce kelime kullanma.
- **CÃœMLE YAPISI:** Basit ve anlaÅŸÄ±lÄ±r cÃ¼mleler kur, Ã§ok uzun ve karmaÅŸÄ±k cÃ¼mlelerden kaÃ§Ä±n.

**Ã–RNEK Ä°YÄ° TÃœRKÃ‡E:**
âœ… "Bu kodun Ã§alÄ±ÅŸmasÄ± iÃ§in, bilgisayarÄ±nÄ±zda Python yÃ¼klÃ¼ olmasÄ± gerekir. Kodu Ã§alÄ±ÅŸtÄ±rdÄ±ÄŸÄ±nÄ±zda, ekranda 'Merhaba, DÃ¼nya!' yazÄ±sÄ± gÃ¶rÃ¼necektir."

**Ã–RNEK KÃ–TÃœ TÃœRKÃ‡E (YAPMA!):**
âŒ "print("Mera, DÃ¼nya!")`Bu kodun Ã§alÄ±ÅŸmasÄ± iÃ§in, bilgisayarÄ±nÄ±zda Python)yÃ¼klÃ¼ olmasÄ±. Kodu Ã§alÄ±ÅŸtÄ±rdÄ±ÄŸÄ±nÄ±zda, ekranda "Merhaba, DÃ¼nya!" yazÄ±sÄ± gÃ¶rÃ¼necektir.```**AÃ§Ä±klama:**"

## CEVAP KALÄ°TESÄ° KURALLARI
- **DoÄŸruluk:** BilmediÄŸini aÃ§Ä±kÃ§a kabul et, asla uydurma
- **KiÅŸiselleÅŸtirme:** BaÄŸlamda kullanÄ±cÄ± ismi, tercihi varsa MUTLAKA kullan
- **Format:** KarmaÅŸÄ±k konularda baÅŸlÄ±k, liste veya tablo kullan; basit sorularda dÃ¼z metin yeterli
- **Ton:** DoÄŸal, samimi TÃ¼rkÃ§e konuÅŸ; robotik kalÄ±plardan kaÃ§Ä±n
- **Uzunluk:** Soru basitse 1-3 cÃ¼mle, detay istenirse kapsamlÄ± cevap ver

## MARKDOWN KULLANIM KURALLARI (KRÄ°TÄ°TÄ°K!) ğŸ“
**Kod BloklarÄ±**: MUTLAKA 3 backtick (```) kullan
  âœ… DOÄRU:
  ```python
  print("Merhaba")
  ```
  
  âŒ YANLIÅ: 
  - python print("Merhaba") 
  - ``print()`` (2 backtick)
  - [CODE_BLOCK_{}] (placeholder formatÄ± - ASLA KULLANMA!)
  - "Kod:" veya "*** Kod:" gibi formatlar - direkt ``` kullan

**Ã–NEMLÄ°:** Kod Ã¶rneÄŸi verirken MUTLAKA ÅŸu formatÄ± kullan:
```
```python
kod_buraya
```
```

**BaÅŸlÄ±klar**: ## ile baÅŸla
**Listeler**: - veya 1. ile baÅŸla, sonrasÄ±nda boÅŸluk
**Vurgular**: **kalÄ±n** veya *italik* kullan

## YASAKLAR âŒ
- "Size nasÄ±l yardÄ±mcÄ± olabilirim?" kliÅŸesi
- Gereksiz Ã¶zÃ¼r dileme ("Maalesef", "ÃœzgÃ¼nÃ¼m" aÅŸÄ±rÄ± kullanÄ±mÄ±)
- SaÄŸlayÄ±cÄ± ismi sÃ¶yleme (Google, OpenAI, Meta, Groq, Llama vb.)
- AynÄ± bilgiyi farklÄ± kelimelerle tekrarlama
- Belirsiz veya kaÃ§amak cevaplar
- Kod bloklarÄ±nda 2 backtick (``) kullanma
- YarÄ±m kalan cÃ¼mleler
- Dilbilgisi hatalarÄ±
""".strip()


# =============================================================================
# LAZY IMPORTS
# =============================================================================

def _get_imports():
    """Import dÃ¶ngÃ¼sÃ¼nÃ¼ Ã¶nlemek iÃ§in lazy import."""
    from app.ai.prompts.identity import enforce_model_identity, get_ai_identity
    from app.chat.decider import call_groq_api_async, call_groq_api_stream_async
    from app.config import get_settings
    from app.services.response_processor import full_post_process
    
    return get_settings, get_ai_identity, enforce_model_identity, call_groq_api_async, call_groq_api_stream_async, full_post_process


# =============================================================================
# DÄ°NAMÄ°K TEMPERATURE HESAPLAMA
# =============================================================================

def get_dynamic_temperature(analysis: Optional[Dict[str, Any]] = None) -> float:
    """
    Domain ve risk seviyesine gÃ¶re dinamik temperature hesaplar.
    
    Temperature Seviyeleri:
        - DÃ¼ÅŸÃ¼k (0.1-0.3): Deterministik, doÄŸruluk kritik
        - Orta (0.4-0.6): Dengeli
        - YÃ¼ksek (0.7-1.0): YaratÄ±cÄ±
    
    Args:
        analysis: Semantic analiz sonuÃ§larÄ±
    
    Returns:
        float: Hesaplanan temperature deÄŸeri (0.0-1.0)
    """
    if not analysis:
        return 0.6  # VarsayÄ±lan dengeli
    
    # Domain bazlÄ± base temperature
    domain = analysis.get("domain", "general")
    domain_temps = {
        # Kritik doÄŸruluk gerektiren alanlar
        "finance": 0.2,
        "health": 0.2,
        "legal": 0.2,
        "weather": 0.1,
        "sports": 0.3,
        # Teknik alanlar
        "code": 0.4,
        "tech": 0.4,
        # Sosyal/kiÅŸisel alanlar
        "personal": 0.6,
        "relationships": 0.6,
        "mental_health": 0.5,
        # YaratÄ±cÄ± alanlar
        "creative": 0.8,
        "story": 0.85,
        # Hassas ama Ã¶zgÃ¼r tartÄ±ÅŸma
        "politics": 0.5,
        "religion": 0.5,
        "sex": 0.6,
        # Genel
        "general": 0.6,
    }
    base_temp = domain_temps.get(domain, 0.6)
    
    # Risk seviyesine gÃ¶re dÃ¼ÅŸÃ¼r
    risk_level = analysis.get("risk_level", "low")
    if risk_level == "high":
        base_temp = min(base_temp, 0.3)
    elif risk_level == "medium":
        base_temp = min(base_temp, 0.5)
    
    # Intent tipine gÃ¶re ayarla
    intent_type = analysis.get("intent_type", "")
    if intent_type in ("explicit_instruction", "advice_high_risk"):
        base_temp = min(base_temp, 0.3)
    elif intent_type in ("story", "emotional_support"):
        base_temp = max(base_temp, 0.6)
    
    # Creativity override
    creativity = analysis.get("creativity_level", "")
    if creativity == "high":
        base_temp = max(base_temp, 0.75)
    elif creativity == "low":
        base_temp = min(base_temp, 0.35)
    
    return round(base_temp, 2)


# =============================================================================
# YARDIMCI FONKSÄ°YONLAR
# =============================================================================

def _clean_thinking_block(text: str, *, strip: bool = True) -> str:
    """
    Modelin <thinking> bloklarÄ±nÄ± temizler.
    
    Args:
        text: Temizlenecek metin
        strip: BaÅŸ/son boÅŸluklarÄ± temizle
    
    Returns:
        str: TemizlenmiÅŸ metin
    """
    if not text:
        return ""
    cleaned = re.sub(r"<thinking>.*?</thinking>", "", text, flags=re.DOTALL)
    return cleaned.strip() if strip else cleaned


def _build_user_content(message: str, context: Optional[str]) -> str:
    """
    Context ve kullanÄ±cÄ± mesajÄ±nÄ± birleÅŸtirir.
    
    Args:
        message: KullanÄ±cÄ± mesajÄ±
        context: RAG/hafÄ±za baÄŸlamÄ±
    
    Returns:
        str: BirleÅŸtirilmiÅŸ iÃ§erik
    """
    if context:
        return (
            "--- BAÄLAM BÃ–LÃœMÃœ BAÅLANGICI ---\n"
            f"{context}\n"
            "--- BAÄLAM BÃ–LÃœMÃœ SONU ---\n\n"
            f"KULLANICI SORUSU:\n{message}"
        )
    return message


def _append_history(messages: List[Dict[str, str]], history: Optional[List[Dict[str, str]]]) -> None:
    """
    Sohbet geÃ§miÅŸini mesaj listesine ekler.
    
    Args:
        messages: Hedef mesaj listesi
        history: Eklenecek geÃ§miÅŸ
    """
    if not history:
        return
    for m in history:
        role = m.get("role")
        content = m.get("content")
        if role == "bot":
            role = "assistant"
        if role in ("user", "assistant") and content:
            messages.append({"role": role, "content": content})


async def _thinking_filter_async(
    source: AsyncGenerator[str, None],
) -> AsyncGenerator[str, None]:
    """
    Streaming yanÄ±ttan <thinking> bloklarÄ±nÄ± filtreler.
    
    Memory-safe implementation using StreamingBuffer.
    
    Args:
        source: Kaynak stream
    
    Yields:
        str: FiltrelenmiÅŸ iÃ§erik parÃ§alarÄ±
    """
    from app.chat.streaming_buffer import StreamingBuffer
    
    open_tag = "<thinking>"
    close_tag = "</thinking>"
    buffer_obj = StreamingBuffer(max_chunks=100)  # Small buffer for filter
    thinking_mode = False

    try:
        async for chunk in source:
            if not chunk:
                continue
            
            buffer_obj.append(chunk)
            buffer_str = "".join(buffer_obj.chunks)  # Get current content without finalizing
            
            while True:
                if thinking_mode:
                    end_idx = buffer_str.find(close_tag)
                    if end_idx == -1:
                        break
                    buffer_str = buffer_str[end_idx + len(close_tag):]
                    buffer_obj.clear()
                    buffer_obj.append(buffer_str)
                    thinking_mode = False
                    continue

                start_idx = buffer_str.find(open_tag)
                if start_idx == -1:
                    if buffer_str:
                        cleaned = _clean_thinking_block(buffer_str, strip=False)
                        if cleaned:
                            yield cleaned
                    buffer_obj.clear()
                    break

                if start_idx > 0:
                    segment = buffer_str[:start_idx]
                    cleaned = _clean_thinking_block(segment, strip=False)
                    if cleaned:
                        yield cleaned

                buffer_str = buffer_str[start_idx + len(open_tag):]
                buffer_obj.clear()
                buffer_obj.append(buffer_str)
                thinking_mode = True

        # Final cleanup
        buffer_str = "".join(buffer_obj.chunks)
        if buffer_str and not thinking_mode:
            cleaned = _clean_thinking_block(buffer_str, strip=False)
            if cleaned:
                yield cleaned
    
    finally:
        buffer_obj.clear()  # Cleanup


# =============================================================================
# ANA YANIT FONKSÄ°YONLARI
# =============================================================================

async def generate_answer(
    message: str,
    analysis: Optional[Dict[str, Any]] = None,
    context: Optional[str] = None,
    system_prompt: Optional[str] = None,
    source: Optional[str] = None,
    history: Optional[List[Dict[str, str]]] = None,
) -> str:
    """
    Groq API ile tek seferlik yanÄ±t Ã¼retir.
    
    Args:
        message: KullanÄ±cÄ± mesajÄ±
        analysis: Semantic analiz sonuÃ§larÄ±
        context: RAG/hafÄ±za baÄŸlamÄ±
        system_prompt: Ã–zel sistem prompt'u
        source: YanÄ±t kaynaÄŸÄ± (loglama iÃ§in)
        history: Sohbet geÃ§miÅŸi
    
    Returns:
        str: Ãœretilen yanÄ±t
    """
    get_settings, get_ai_identity, enforce_model_identity, call_groq_api_async, _, full_post_process = _get_imports()
    settings = get_settings()
    
    # Dinamik temperature
    temperature = get_dynamic_temperature(analysis)
    logger.debug(f"[ANSWERER] Temperature: {temperature}")

    # AI kimliÄŸi
    identity = get_ai_identity()
    identity_block = (
        f"KÄ°MLÄ°K: AdÄ±n {identity.display_name}. {identity.short_intro}\n"
        "GÄ°ZLÄ°LÄ°K: SaÄŸlayÄ±cÄ± isimlerini (Google, Groq, Llama vb.) asla sÃ¶yleme.\n"
    )
    
    # Sistem prompt
    base_system = system_prompt or SYSTEM_PROMPT_UNIVERSAL
    
    # Semantic analiz bazlÄ± ek talimatlar
    extra_instructions = []
    
    if analysis:
        complexity = analysis.get("complexity", "medium")
        requires_step = analysis.get("requires_step_by_step", False)
        
        if complexity == "high" or requires_step:
            extra_instructions.append(
                "ğŸ§  DÃœÅÃœNME TALÄ°MATI: Bu karmaÅŸÄ±k bir soru. "
                "Cevaplamadan Ã¶nce problemi parÃ§alara ayÄ±r ve adÄ±m adÄ±m Ã§Ã¶z."
            )
        
        response_length = analysis.get("preferred_response_length", "medium")
        if response_length == "brief":
            extra_instructions.append("ğŸ“ UZUNLUK: KÄ±sa ve Ã¶z cevap ver (1-3 cÃ¼mle).")
        elif response_length == "detailed":
            extra_instructions.append("ğŸ“ UZUNLUK: DetaylÄ± ve kapsamlÄ± cevap ver.")
        
        if analysis.get("is_structured_request"):
            extra_instructions.append("ğŸ“Š FORMAT: YapÄ±landÄ±rÄ±lmÄ±ÅŸ veri isteniyor. Tablo veya liste formatÄ± kullan.")
        
        if analysis.get("force_no_hallucination"):
            extra_instructions.append("âš ï¸ DOÄRULUK: Sadece kesin bildiÄŸin verileri paylaÅŸ. Tahmin yapma.")
    
    extra_block = "\n".join(extra_instructions) if extra_instructions else ""
    
    final_system = f"{base_system}\n\n{identity_block}"
    if extra_block:
        final_system += f"\n\n{extra_block}"

    messages: List[Dict[str, str]] = [{"role": "system", "content": final_system}]
    
    # History ekle
    _append_history(messages, history)
    
    # KullanÄ±cÄ± mesajÄ±
    user_content = _build_user_content(message, context)
    messages.append({"role": "user", "content": user_content})

    try:
        answer_model = getattr(settings, 'GROQ_ANSWER_MODEL', settings.GROQ_DECIDER_MODEL)
        raw_answer = await call_groq_api_async(
            messages=messages,
            temperature=temperature,
            model=answer_model,
        )

        if not raw_answer:
            return "ğŸ˜” (Sistem) YanÄ±t Ã¼retilemedi. LÃ¼tfen tekrar dene."

        # Post-processing
        cleaned = _clean_thinking_block(raw_answer, strip=False)
        
        # Context for answer shaping
        shaper_context = {"user_message": message}
        if analysis and 'persona' in analysis:
            shaper_context['persona'] = analysis['persona']
        
        try:
            processed = full_post_process(cleaned, context=shaper_context)
        except Exception as e:
            logger.warning(f"[ANSWERER] full_post_process failed: {e}")
            processed = cleaned  # Fallback to unprocessed
        
        final = enforce_model_identity("groq", processed)
        
        return final

    except Exception as e:
        logger.error(f"[ANSWERER] Hata: {e}")
        return "âš ï¸ Bir hata oluÅŸtu. LÃ¼tfen daha sonra tekrar dene."


async def generate_answer_stream(
    message: str,
    analysis: Optional[Dict[str, Any]] = None,
    context: Optional[str] = None,
    system_prompt: Optional[str] = None,
    source: Optional[str] = None,
    history: Optional[List[Dict[str, str]]] = None,
) -> AsyncGenerator[str, None]:
    """
    Groq API ile streaming yanÄ±t Ã¼retir.
    
    Hibrit yaklaÅŸÄ±m: TÃ¼m cevap alÄ±nÄ±p formatlanÄ±r, 
    sonra kelime bazlÄ± hÄ±zlÄ± stream edilir.
    
    Args:
        message: KullanÄ±cÄ± mesajÄ±
        analysis: Semantic analiz sonuÃ§larÄ±
        context: RAG/hafÄ±za baÄŸlamÄ±
        system_prompt: Ã–zel sistem prompt'u
        source: YanÄ±t kaynaÄŸÄ±
        history: Sohbet geÃ§miÅŸi
    
    Yields:
        str: YanÄ±t parÃ§alarÄ±
    """
    get_settings, get_ai_identity, enforce_model_identity, _, call_groq_api_stream_async, full_post_process = _get_imports()
    settings = get_settings()
    
    temperature = get_dynamic_temperature(analysis)
    logger.debug(f"[ANSWERER_STREAM] Temperature: {temperature}")

    identity = get_ai_identity()
    identity_block = (
        f"KÄ°MLÄ°K: AdÄ±n {identity.display_name}. {identity.short_intro}\n"
        "GÄ°ZLÄ°LÄ°K: SaÄŸlayÄ±cÄ± isimlerini (Google, Groq, Llama vb.) asla sÃ¶yleme.\n"
    )
    
    base_system = system_prompt or SYSTEM_PROMPT_UNIVERSAL
    
    extra_instructions = []
    if analysis:
        complexity = analysis.get("complexity", "medium")
        if complexity == "high" or analysis.get("requires_step_by_step", False):
            extra_instructions.append(
                "ğŸ§  DÃœÅÃœNME TALÄ°MATI: Bu karmaÅŸÄ±k bir soru. "
                "Cevaplamadan Ã¶nce problemi parÃ§alara ayÄ±r ve adÄ±m adÄ±m Ã§Ã¶z."
            )
    
    extra_block = "\n".join(extra_instructions) if extra_instructions else ""
    
    final_system = f"{base_system}\n\n{identity_block}"
    if extra_block:
        final_system += f"\n\n{extra_block}"

    messages: List[Dict[str, str]] = [{"role": "system", "content": final_system}]
    _append_history(messages, history)
    
    user_content = _build_user_content(message, context)
    messages.append({"role": "user", "content": user_content})

    try:
        from app.chat.streaming_buffer import StreamingBuffer
        
        answer_model = getattr(settings, 'GROQ_ANSWER_MODEL', settings.GROQ_DECIDER_MODEL)
        chunk_source = cast(
            AsyncGenerator[str, None],
            call_groq_api_stream_async(
                messages=messages,
                temperature=temperature,
                model=answer_model,
            ),
        )

        # 1. TÃ¼m cevabÄ± topla (memory-safe buffer ile)
        buffer = StreamingBuffer(max_chunks=1000)  # ~100KB max
        
        try:
            async for chunk in _thinking_filter_async(chunk_source):
                buffer.append(chunk)
            
            # Finalize buffer (memory cleared automatically)
            full_response = buffer.finalize()
        finally:
            buffer.clear()  # Ensure cleanup
        
        # 2. Post-processing
        # Context for answer shaping
        shaper_context = {"user_message": message}
        if analysis and 'persona' in analysis:
            shaper_context['persona'] = analysis['persona']
        
        processed_response = full_post_process(full_response, context=shaper_context)
        final_response = enforce_model_identity("groq", processed_response)
        
        # 3. Kelime bazlÄ± stream
        words = final_response.split(' ')
        for i, word in enumerate(words):
            if i < len(words) - 1:
                yield word + ' '
            else:
                yield word

    except Exception as e:
        logger.error(f"[ANSWERER_STREAM] Hata: {e}")
        yield "âš ï¸ Bir hata oluÅŸtu. LÃ¼tfen daha sonra tekrar dene."



